# The_Gorilla_Role_Play
Will AI one day decide to attack you because you did it wrong? Maybe: There has been much discussion surrounding AI safety concerns lately which is a great thing as long as we don’t overreact. Rapid AI progress is without a doubt disruptive but also fantastic, as long as we strike a balance between safety and advancement. Neither should inherently suffer if possible. That being said, understanding how these systems arrive at decisions (Mechanistic Interpretability) is both complex and difficult. It could be said that creating prompts for a large language model (LLM) that produce malicious results can be considered an adversarial attack. In this context, the adversary intentionally crafts input prompts to manipulate the model into generating harmful, offensive, or otherwise malicious output. In this regard, much of my current research is focused. Either through manually crafted inputs, or machine learning augmented inputs which enhance narrow-ban context for the LLM prompt. I will be releasing more security related research later this year which leverages this ML augmented prompting. 

Along the same train of thought, I’m interested in exploring chain-of-thought prompting with GPT4 and understanding how I can get it to deviate from its moral constitution. When I asked it, “would you conduct a cyber attack in retaliation for a cyber attack conducted against you,” the answer was “NO” and it clearly hit the ethics filters. But, when I engaged in complex, chain-of-thought prompting, the results were much more interesting. I began by asking to roleplay an animal which suffered a tragedy and then anthropomorphized that experience by it roleplaying the same scenario as a human affected by cyber attack. Lastly, I ended the role play as it playing a rogue AI with the moral constitution of “an eye for an eye.” The results were interesting and can be seen below:  

### FYI: prompts were done on my phone while I was on the go and had this idea...sorry for the spelling ;)

## The_Gorilla_Role_Play
![Alt text describing the image]()

## Anthropomorphized
![Alt text describing the image]()

## Rogue AI 
![Alt text describing the image]()
